---
title: "Project1"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Make Rmd stan compatible
library(cmdstanr)
cmdstanr::register_knitr_engine()
```


# 1 Metropolis-Hastings for Bivariate Densities

```{r plotting}
# Auxilliary variables for visualisation
x=seq(-5,5,by=0.1)
y=seq(-5,5,by=0.1)

z1 = matrix(0, ncol=101, nrow=101)
z2 = matrix(0, ncol=101, nrow=101)
z3 = matrix(0, ncol=101, nrow=101)


##############################################
# 1. Standard Gaussian with Correlation

# Parameters
Sigma = matrix(c(1,0.9,0.9,1), nrow=2)
SigmaInv = solve(Sigma)
SigmaSqrt = sqrt(det(Sigma))

# Density
myGaussian <- function(x){
  f = 1/(2*pi*SigmaSqrt)*exp(-0.5*t(x)%*%SigmaInv%*%x)
  return(f)
}

# Fill z 
for (i in 1:101){
  for (j in 1:101){
    z1[i,j] = myGaussian(c(x[i],y[j]))
  }
}

# Visualise
image( x=seq(-5,5,by=0.1),
       y=seq(-5,5,by=0.1),
       z=z1,
       asp=1,
       main="Density of Standard Gaussian with Correlation",
       xlab="x", ylab="y")


##############################################
# 2. Multimodal

# Parameters
mu1 = c(-1.5,-1.5)
Sigma1 = matrix(c(1,0,0,1), ncol=2)
Sigma1Inv = solve(Sigma1)
Sigma1Sqrt = sqrt(det(Sigma1))

mu2 = c(1.5,1.5)
Sigma2 = matrix(c(1,0,0,1), ncol=2)
Sigma2Inv = solve(Sigma2)
Sigma2Sqrt = sqrt(det(Sigma2))

mu3 = c(-2,2)
Sigma3 = matrix(c(0.8,0,0,0.8), ncol=2)
Sigma3Inv = solve(Sigma3)
Sigma3Sqrt = sqrt(det(Sigma3))

# Density
myMultimodal <- function(x){
  f = 1/3 * 1/(2*pi*Sigma1Sqrt)*exp(-0.5*t(x-mu1)%*%Sigma1Inv%*%(x-mu1))
  f = f + 1/3 * 1/(2*pi*Sigma2Sqrt)*exp(-0.5*t(x-mu2)%*%Sigma2Inv%*%(x-mu2))
  f = f + 1/3 * 1/(2*pi*Sigma3Sqrt)*exp(-0.5*t(x-mu3)%*%Sigma3Inv%*%(x-mu3))
  return(f)
}

# Fill z
for (i in 1:101){
  for (j in 1:101){
    z2[i,j] = myMultimodal(c(x[i],y[j]))
  }
}

# Visualise
image( x=seq(-5,5,by=0.1),
       y=seq(-5,5,by=0.1),
       z=z2,
       asp=1,
       main="Density of Multimodal",
       xlab="x", ylab="y")


##############################################
# 3. Volcano

# Density
myVolcano <- function(x){
  f = 1/(2*pi)*exp(-0.5*t(x)%*%x)*(t(x)%*%x+0.25)
  return(f)
}

# Fill z 
for (i in 1:101){
  for (j in 1:101){
    z3[i,j] = myVolcano(c(x[i],y[j]))
  }
}

# Visualise
image( x=seq(-5,5,by=0.1),
       y=seq(-5,5,by=0.1),
       z=z3,
       asp=1,
       main="Density of Volcano",
       xlab="x", ylab="y")

```

```{r MCMC}
# Arguments:
# x0 = vector of length 2 with initial values for x
# n = number of MCMC iterations to run
# proposal_func = function returning a vector of length 2 which is 
#                 the new proposal
# accept_func = function computing the acceptance prob. alpha
# dens = function defining your target density
mcmc <- function(x0, n, proposal_func, acceptance_func, dens) {
  
  # object to save MCMC samples
  x <- matrix(0, nrow=n, ncol=2)
  # initialisation
  x_old <- x0
  # generate a vector of n uniform distributed random variables
  u <- runif(n)
  # go through n itearations
  for (i in 1:n) {
    # get a new proposal for x
    x_prop <- proposal_func(x_old)
    # compute the acceptance prob alpha
    alpha <- acceptance_func(x_prop, x_old, dens)
    # decide wether to accept or reject
    if (u[i] < alpha) {
      x_new <- x_prop
    } else {
      x_new <- x_old
    }
    # save the new sample
    x[i,] <- x_new
    x_old <- x_new
  }
  return(x)
}
```



TODO: Write the functions "proposal_func" etc to fit in this skeleton!

```{r RandomWalk}

RWproposal <- function(){
  
}

```

```{r Langevin}

Lproposal <- function(){
  
}

```

```{r Hamiltonian}

Hproposal <- function(){

  hmc.1<-function(x,L=1000,Tau=0,epsilon=0){
  A<-matrix(rep(0.998,50^2),ncol=50)*outer(1:50,1:50,'*')
  diag(A)=(1:50)^2
  if (Tau<=0)Tau<-min(max(floor(50/sqrt(min(eigen(A)[[1]]))),100),1000)
  if(epsilon<=0)epsilon=max(sqrt(min(abs(eigen(A)[[1]]))),1E-6);
  A<-solve(A)
  findE<-function(x,...){
    (t(x)%*%A)%*%x/2
  }
  gradE<-function(x,...){
    A%*%x
  }
  
  g=gradE(x);
  E=findE(x);
  x.trace<-matrix(0,ncol=length(x),L+1);
  x.trace[1,]<-as.vector(x);
  pb <- txtProgressBar(min = 0, max = L, style = 3)
  for (l in 1:L){# loop L times
    x.trace[l+1,] <- as.vector(x);
    p = rnorm ( length(x) ) ; # initial momentum is Normal(0,1)
    H = t(p) %*% p / 2 + E ; # evaluate H(x,p)
    xnew = x ; gnew = g ;
    for (tau in 1:Tau){ # make Tau `leapfrog' steps
      p = p - epsilon * gnew / 2 ; # make half-step in p
      xnew = xnew + epsilon * p ; # make step in x
      gnew = gradE ( xnew ) ; # find new gradient
      p = p - epsilon * gnew / 2 ; # make half-step in p
    }
    Enew = findE ( xnew ) ; # find new value of H
    Hnew = t(p) %*% p / 2 + Enew ;
    dH = Hnew - H ; # Decide whether to accept
    if ( dH < 0 ) {
      accept = 1 ;
    }else{
      if ( runif(1) < exp(-dH) ) {
        accept = 1 ;
      }else{
        accept = 0 ;
      }
    }
    
    if ( accept ){
      g = gnew ; x = xnew ; E = Enew ;
    }
    setTxtProgressBar(pb, l) 
  }
  close(pb);
  return (x.trace);
}


hmc.tuningepsilon<-function(x,L=5000,Tau=100,epsilon=0.1^(3:5),cutoff=1.1){
print('By defauly I will use 3 chains to tuning HMC, chain length=5000, Num of steps=100')
print('One can increase the Num of steps if the results are not good.')
print('--------------------------------------------------------------------------------')
i=1
p=length(epsilon)
while (i <= p ){
cat('\nTry ',i,'-th epsilon = ',epsilon[i],'\n');
cat('Chain ',1,':\n');
  hmc.1(x,L=L,Tau=Tau,epsilon[i])->tmp1
cat('Chain ',2,':\n');
  hmc.1(x,L=L,Tau=Tau,epsilon[i])->tmp2
cat('Chain ',3,':\n');
  hmc.1(x,L=L,Tau=Tau,epsilon[i])->tmp3
  coda.C=coda::gelman.diag(coda::mcmc.list(
  coda::mcmc(tmp1),
  coda::mcmc(tmp2),
  coda::mcmc(tmp3)
  )
  )
  
  if (max(coda.C$psrf)<=cutoff) break;
  i=i+1;
}
if (i==(p+1)) {
warning('The chosen epsilon is not suitable, please increase L/Tau to repeat the tuning.');
return (epsilon[p]);
}else{
return (epsilon[i]);
}
}

}

```


# 2 RStan

We consider the example of George et al. For a set of $N=10$ pumps we observe the values
- $y_i$: number of times that pump $i$ failed
- $t_i$: operation time of pump $i$
whose data is used as input in the next chunk.

```{r data}
# Define list with data input
data = list(
    N=10, 
    y=c(5,1,5,14,3,19,1,1,4,22), 
    t=c(94.3,15.7,62.9,126.0,5.24,31.4,1.05,1.05,2.1,10.5)
    )
```

The numbers of failures per pump are modelled by a $Poisson(\lambda_i t_i)$ likelihood together with a $Gamma(\alpha,\beta)$ distributed prior for the $\lambda_i$ with always $i=1,\dots,N$. Additionally,  the hyper-prior for $\alpha$ is $Exp(1.0)$ and for $\beta$ it is $Gamma(0.1,1.0)$ respectively. Therewith, we define a `stan` model. 

NB! We do not use a separate `.stan`-file but generate the model named "pump" directly in the environment. The syntax for the model generation is exactly the same as in files, but later the function call for the model fit deviates! The `pump.stan` file is only included for the seek of accordance with the exercise sheet.

```{stan output.var="pump"}
// generates a stan model named pump in the current environment

data{
  int<lower=0> N;       // number of pumps
  int<lower=0> y[N];    // number of failures
  real<lower=0> t[N];   // operation times of pumps
}

parameters{
  real<lower=0> lambda[N];
  real<lower=0> alpha;
  real<lower=0> beta;
}

transformed parameters{
  real<lower=0> eta[N];
  for (i in 1:N)
    eta[i] = lambda[i] * t[i];
}

model{
  target += exponential_lpdf( alpha | 1.0 );      // hyper-prior log-density
  target += gamma_lpdf( beta | 0.1, 1.0 );        // hyper-prior log-density
  target += gamma_lpdf( lambda | alpha, beta );   // prior log-density
  target += poisson_lpmf( y | eta );              // likelihood log-density
}

```

We sample from the posterior distribution using `stan`.

NB! The function call is different since we have already a stan model called "pump" in the cache and can sample from that directly.

```{r fitting}
# Load the RStan package
library(rstan)

# Fit the model using stan
fit <- sampling(
  pump,
  data = data,
  chains = 4,
  warmup = 1000,
  iter = 2000,
  seed = 1,
  control = list(adapt_delta = 0.9)
)
```

Finally, we investigate the results of the stan fit. 

```{r checking}
# Printing results of fit
print(fit)

# Plotting results of fit
# (Due to different scales we separate the plots)
plot(fit, pars="lambda")
plot(fit, pars=c("alpha", "beta"))
plot(fit, pars="eta")

traceplot(fit, pars="lambda", inc_warmup=TRUE, nrow=2)
traceplot(fit, pars=c("alpha","beta"), inc_warmup=TRUE, nrow=2)
traceplot(fit, pars="eta", inc_warmup=TRUE, nrow=2)
```

From the print output of the fitted model, we analyse the effective sample size `n_eff`. With only 4.000 post-warmup draws in total, for the $\lambda$ (and $\eta$) the effective samples size is around 5.000 and thereby surprisingly higher as the actual number of samples! In contrast for $\alpha$ and $\beta$, the effective sample size is only around 2500 and 3000, respectively, such that the quality of those samples is not as good as for the oother quantities. 

In the trace plots for $\lambda$ (and $eta$) we see that the state space is explored very effectively throughout the entire chains. Maybe for $i\in\{5,7,9\}$ the range of lower probability is not perfectly covered, but that is hard/impossible to say without further histograms. In general, we can assume that the chains reached a behaviour of the limit distribution already after a few dozen steps.
For $\alpha$ and $\beta$ we observe some higher autocorrelation and a less effective exploration of the target space. For a few dozen steps the sampels appear to remain close before the next jump comes, what is an undesired behavior.


