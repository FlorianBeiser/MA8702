---
title: "Project1"
subtitle: "Markov Chain Monte Carlo techniques"
author: "Florian Beiser, Yaolin Ge"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Make Rmd stan compatible
# library(cmdstanr)
# cmdstanr::register_knitr_engine()
```


# 1 Metropolis-Hastings for Bivariate Densities

We consider three different bivariate densities.
1. Standard Gaussian with correlation
2. Multimodal as muixture of three Gaussians
3. volcano (unnormalized)

## 1.1 Plotting

We start the project with the visulaisation of the respective densities in $[-5,5]\times[-5,5]$. (Whyever R likes to put more white space left and right of the plots, but the middle areas are still representative.)

```{r plotting, fig.height=4, fig.width=12}
# Auxilliary variables for visualisation
x=seq(-5,5,by=0.1)
y=seq(-5,5,by=0.1)

z1 = matrix(0, ncol=101, nrow=101)
z2 = matrix(0, ncol=101, nrow=101)
z3 = matrix(0, ncol=101, nrow=101)

par(mfrow=c(1,3))

##############################################
# 1. Standard Gaussian with Correlation

# Parameters
Sigma = matrix(c(1,0.9,0.9,1), nrow=2)
SigmaInv = solve(Sigma)
SigmaSqrt = sqrt(det(Sigma))

# Density
myGaussian <- function(x){
  f = 1/(2*pi*SigmaSqrt)*exp(-0.5*t(x)%*%SigmaInv%*%x)
  return(f)
}

# Fill z 
for (i in 1:101){
  for (j in 1:101){
    z1[i,j] = myGaussian(c(x[i],y[j]))
  }
}

# Visualise
image( x=x, y=y, z=z1,
       asp=1, xlab="x", ylab="y",
       main="Analytical Density of Standard Gaussian with Correlation")


##############################################
# 2. Multimodal

# Parameters
mu1 = c(-1.5,-1.5)
Sigma1 = matrix(c(1,0,0,1), ncol=2)
Sigma1Inv = solve(Sigma1)
Sigma1Sqrt = sqrt(det(Sigma1))

mu2 = c(1.5,1.5)
Sigma2 = matrix(c(1,0,0,1), ncol=2)
Sigma2Inv = solve(Sigma2)
Sigma2Sqrt = sqrt(det(Sigma2))

mu3 = c(-2,2)
Sigma3 = matrix(c(0.8,0,0,0.8), ncol=2)
Sigma3Inv = solve(Sigma3)
Sigma3Sqrt = sqrt(det(Sigma3))

# Density
myMultimodal <- function(x){
  f = 1/3 * 1/(2*pi*Sigma1Sqrt)*exp(-0.5*t(x-mu1)%*%Sigma1Inv%*%(x-mu1))
  f = f + 1/3 * 1/(2*pi*Sigma2Sqrt)*exp(-0.5*t(x-mu2)%*%Sigma2Inv%*%(x-mu2))
  f = f + 1/3 * 1/(2*pi*Sigma3Sqrt)*exp(-0.5*t(x-mu3)%*%Sigma3Inv%*%(x-mu3))
  return(f)
}

# Fill z
for (i in 1:101){
  for (j in 1:101){
    z2[i,j] = myMultimodal(c(x[i],y[j]))
  }
}

# Visualise
image( x=x, y=y, z=z2,
       asp=1, xlab="x", ylab="y",
       main="Analytical Density of Multimodal")


##############################################
# 3. Volcano

# Density
myVolcano <- function(x){
  f = 1/(2*pi)*exp(-0.5*t(x)%*%x)*(t(x)%*%x+0.25)
  return(f)
}

# Fill z 
for (i in 1:101){
  for (j in 1:101){
    z3[i,j] = myVolcano(c(x[i],y[j]))
  }
}

# Visualise
image( x=x, y=y, z=z3,
       asp=1, xlab="x", ylab="y",
       main="Analystical Density of Volcano")

```

For the subsequent part, where different Metropolis Hastings MCMC algorithms will be implemented, we prepare with the skeleton that was discussed in the exercise class and uploaded. This defines the frame for all MCMC implementations under consideration which differ in the `proposal_func` and `acceptance_func`.  

```{r MCMC}
# Arguments:
# x0 = vector of length 2 with initial values for x
# n = number of MCMC iterations to run
# proposal_func = function returning a vector of length 2 which is 
#                 the new proposal
# accept_func = function computing the acceptance prob. alpha
# dens = function defining your target density
mcmc <- function(x0, n, proposal_func, acceptance_func, dens) {
  
  # object to save MCMC samples
  x <- matrix(0, nrow=n, ncol=2)
  alphas <- rep(0,n)
  # initialisation
  x_old <- x0
  # generate a vector of n uniform distributed random variables
  u <- runif(n)
  # go through n itearations
  for (i in 1:n) {
    # get a new proposal for x
    x_prop <- proposal_func(x_old, dens)
    # compute the acceptance prob alpha
    alpha <- acceptance_func(x_prop, x_old, dens)
    alphas[i] = alpha
    # decide wether to accept or reject
    if (u[i] < alpha) {
      x_new <- x_prop
    } else {
      x_new <- x_old
    }
    # save the new sample
    x[i,] <- x_new
    x_old <- x_new
  }
  return(list("x"=x,"alphas"=alphas))
}
```

These implementation is not tweaked for optimal performance, e.g. log-scale could lead to better scaling or reduction of redundant calculations in the individual MCMC versions could lead to fast execution, but we want to the same skeleton for all algorithms to higlight similarities and differences. For the analysis of the outcomes, we use below's plotting functionality.

```{r MCMCplotting}
# Arguments:
# trace = trace of MCMC algo [x_1,x_2]^n

# Function:
# - Density plots
# - Trace plots
# - Autocorrelation
MCMCplots <- function(trace, surface=FALSE){
  # Generating empirical densitty
  kd = MASS::kde2d(x=trace$x[,1], y=trace$x[,2], n=101, lims=c(-5,5,-5,5))
  
  # Nice 3D empirical density plots 
  # WARNING: VERY SLOW
  if (surface==TRUE){
    library(plotly)
    plot_ly(x=kd$x, y=kd$y, z=kd$z, type="surface")
  }
  
  layout(matrix(c(1,1,2,3,4,5), 2, 3))
  
  # 2D empirical density plots
  image( x=kd$x, y=kd$y, z=kd$z,
         asp=1, main="Density",
         xlab="x", ylab="y", xlim=c(-5,5), ylim=c(-5,5))
  
  # Trace plots
  plot(trace$x[,1], type="l", ylab="x1", 
     main="Trace of x1", ylim=c(-3.5,3.5))
  plot(trace$x[,2], type="l", ylab="x2", 
     main="Trace of x2", ylim=c(-3.5,3.5))

  # Autocorrelation
  acf(trace$x[,1], main="Autocorrelation for x1")
  acf(trace$x[,2], main="Autocorrelation for x2")
  
  title(paste("Analysis (tuning parameter sigma=",sigma,
               "and mean acceptance rate", sum(trace$alphas)/length(trace$alphas),")"), 
        outer=TRUE)
}

```

We will use the same input parameters for all subsequent MCMC calculations, the tuning parameters are of course not influenced.

```{r MCMCinput}
# Input
x0 = c(0,0)
n = 2000
```


## 1.2 Random walk MH 

The random walk MH uses a symmetric $\mathcal{N}(\texttt{x_old},\sigma^2)$ distribution to generate the new proposal. Moreover, the general MH acceptance probability simplifies.

```{r RandomWalk}
# Loads
library(mvtnorm)

# Arguments: 
# x_old = old state [x_1_old, x_2_old]

# Function:
# Generating proposal following the Random walk MH

# Return:
# x_prop = proposed state [x_1_prop, x_2_prop]
RWproposal <- function(x_old, dens){
  x_prop = c(rmvnorm(1, x_old, sigma**2 * diag(1,2,2)))
  return(x_prop)
}

# Arguments: 
# x_prop = proposed state [x_1_prop, x_2_prop]
# x_old = old state [x_1_old, x_2_old]
# dens = density function of target dist

# Function:
# Calculating acceptance prob for random walk MH

# Return:
# alpha = acceptance probability (numeric)
RWacceptance <- function(x_prop, x_old, dens){
  alpha = min(1, dens(x_prop)/dens(x_old))
  return(alpha)
}
```

### Standard Gaussian

We first test the Random Walk MH for the Gaussian with different tuning parameters $\sigma$ in the proposal distribution. 

```{r RandomWalk1, fig.height=4, fig.width=12}
##############################
# Testing Random Walk for Gaussian

# Tuning parameters
sigma = 0.1
trace0 = mcmc(x0, n, RWproposal, RWacceptance, myGaussian )
MCMCplots(trace0)

sigma = 0.5 
trace1 = mcmc(x0, n, RWproposal, RWacceptance, myGaussian )
MCMCplots(trace1)

sigma = 1.0
trace2 = mcmc(x0, n, RWproposal, RWacceptance, myGaussian )
MCMCplots(trace2)

sigma = 2.5
trace3 = mcmc(x0, n, RWproposal, RWacceptance, myGaussian )
MCMCplots(trace3)

```

Depending on the choice of the tuning parameter the MCMC algorithms show different efficiency. Actually all tuning parameters explore the state-space rather sedately. However, for values $\sigma<0.5$ the mean acceptance rate gets bigger than recommended and the traces change in too little step, for values $\sigma>1.5$ the traces start to pause in a level due to a too small acceptance rate. We recommend values $\sigma\in(0.5,1.5)$ for the Gaussian, since for those values the autocorrelation has t he smallest lag and shrinks at least after 20 steps (what is still much).


### Multimodal

We continue with application of the Random Walk MH to the Multimodal with different tuning parameters $\sigma$ in the proposal distribution. 

```{r RandomWalk2, fig.height=4, fig.width=12}
##############################
# Testing Random Walk for Multimodal

# Tuning parameters
sigma = 0.1
trace1 = mcmc(x0, n, RWproposal, RWacceptance, myMultimodal )
MCMCplots(trace1)

sigma = 1.0
trace2 = mcmc(x0, n, RWproposal, RWacceptance, myMultimodal )
MCMCplots(trace2)

sigma = 5.0
trace3 = mcmc(x0, n, RWproposal, RWacceptance, myMultimodal )
MCMCplots(trace3)

sigma = 10.0
trace3 = mcmc(x0, n, RWproposal, RWacceptance, myMultimodal )
MCMCplots(trace3)

```

For the smallest choice of $\sigma=0.1$ the chain does not really explore all modes and stays for quite long in one mode when it is there, therefore the autocorrelation is very high. When increasing up to $\sigma=5.0$ we improve the exploration of the different modes and reduce the autocorrelation significantly. Moreover, the mean acceptance rate reaches the recommended range. For very high tuning parameter choices $\sigma=10.0$, we start to wildly jump from one mode to the other, but the traces start to pause too long between the jumps. 


### Volcano

Finally, we try the random walk MH for the volcano shaped density.

```{r RandomWalk3, fig.height=4, fig.width=12}
##############################
# Testing Random Walk for Volcano

# Tuning parameters
sigma = 0.1
trace1 = mcmc(x0, n, RWproposal, RWacceptance, myVolcano )
MCMCplots(trace1)

sigma = 0.5
trace2 = mcmc(x0, n, RWproposal, RWacceptance, myVolcano )
MCMCplots(trace2)

sigma = 1.5
trace3 = mcmc(x0, n, RWproposal, RWacceptance, myVolcano )
MCMCplots(trace3)

sigma = 2.5
trace3 = mcmc(x0, n, RWproposal, RWacceptance, myVolcano )
MCMCplots(trace3)

```

If the tuning parameter is chosen too small $\sigma=0.1$ the chain does not explore the entire ring, but get stuck. When increasing $\sigma$ the chain starts to walk along the circle, but for higher parameters like $\sigma=1.5$ the chain explores the ring with a short autocorrelation (figuratively speaking, it can also jump from one side toe the other and does not need to walk along the circle). 


### Conclusion

For all examples, there is a range of runing parameters which explore the state-space, but the autocorrelation is still pretty high.



## 1.3 Langevin MH

Based on the Langevin dynamics and its Euler-Maruyama discretisation, the MALA algorithm uses gradient information to define the proposal density. 

```{r Langevin}
# Loads
library(mvtnorm)

# Arguments:
# x_old = old state [x_1,x_2]
# dens = target density (from local variables)

# Function:
# Using 2nd order central-FD 

# Return:
# Lgrad = grad (log (dens(x_old)))
Lgrad <- function(x_old, dens){
  Lgrad = rep(0,length(x_old))
  h = 1e-3
  e1 = c(1,0)
  e2 = c(0,1)
  Lgrad[1] = (-log(dens(x_old-h*e1)) + log(dens(x_old+h*e1)))/(2*h)
  Lgrad[2] = (-log(dens(x_old-h*e2)) + log(dens(x_old+h*e2)))/(2*h)
  return(Lgrad)
}

# Arguments: 
# x_old = old state [x_1_old, x_2_old]

# Function:
# Generating proposal following the Lnagevin MH

# Return:
# x_prop = proposed state [x_1_prop, x_2_prop]
Lproposal <- function(x_old, dens){
  x_prop =c(rmvnorm(1, x_old + sigma**2/2 * Lgrad(x_old, dens), sigma**2 *diag(1,2,2))) 
  return(x_prop)
}

# Arguments: 
# x = state [x_1, x_2]
# y = condition [y_1,y_2]

# Function:
# Calculating conditional proposal density value 
# for the Langevin proposal Q

# Return:
# f = Q(x|y)
dLproposal <- function(x,y, dens){
  f = dmvnorm(x, y + sigma**2/2 * Lgrad(y, dens), sigma**2 *diag(1,2,2))
  return(f)
}


# Arguments: 
# x_prop = proposed state [x_1_prop, x_2_prop]
# x_old = old state [x_1_old, x_2_old]
# dens = density function of target dist

# Function:
# Calculating acceptance prob for Langevin MH

# Return:
# alpha = acceptance probability (numeric)
Lacceptance <- function(x_prop, x_old, dens){
  alpha = min(1, dLproposal(x_old, x_prop, dens)/dLproposal(x_prop,x_old, dens) * dens(x_prop)/dens(x_old))
  return(alpha)
}
```

### Standard Gaussian

We first test the Langevin MH for the Gaussian with different tuning parameters $\sigma$ in the proposal distribution. 

```{r Langevin1, fig.height=4, fig.width=12}
##############################
# Testing Langevin for Gaussian

# Tuning parameters
sigma = 0.25 
trace1 = mcmc(x0, n, Lproposal, Lacceptance, myGaussian )
MCMCplots(trace1)

sigma = 0.5 
trace2 = mcmc(x0, n, Lproposal, Lacceptance, myGaussian )
MCMCplots(trace2)

sigma = 0.75
trace3 = mcmc(x0, n, Lproposal, Lacceptance, myGaussian )
MCMCplots(trace3)

sigma = 1.0
trace4 = mcmc(x0, n, Lproposal, Lacceptance, myGaussian )
MCMCplots(trace4)

```
Even if the mean acceptance rate for small parameters $\sigma\leq 0.5$ is in the asymptotically optimal range, the trace evolves to slow and with too high correlation. For $\sigma=0.75$ the target density is replicated - the autocorrelation decreases and the explorance is at least acceptable. Already for $\sigma=1.0$ the acceptance rate is too small and the chain pauses too long to explore the space.


### Multimodal

We continue with application of the Langevin MH to the Multimodal with different tuning parameters $\sigma$ in the proposal distribution. 

```{r Langevin2, fig.height=4, fig.width=12}
##############################
# Testing Langevin for Multimodal

# Tuning parameters
sigma = 0.25
trace1 = mcmc(x0, n, Lproposal, Lacceptance, myMultimodal )
MCMCplots(trace1)

sigma = 0.75
trace2 = mcmc(x0, n, Lproposal, Lacceptance, myMultimodal )
MCMCplots(trace2)

sigma = 1.0
trace3 = mcmc(x0, n, Lproposal, Lacceptance, myMultimodal )
MCMCplots(trace3)

sigma = 1.5
trace4 = mcmc(x0, n, Lproposal, Lacceptance, myMultimodal )
MCMCplots(trace4)

```
For small tuning parameter $\sigma\leq 0.75$ the Langevin chain get stuck in the local maxima of the multimodal dsitribution too long and hence does explore the other modes too badly. For higher parameters $\sigma\geq1.0$ the chains reduce their autocorrelation and start to explore all modes. 


### Volcano

Finally, we try the Langevin MH for the volcano shaped density.

```{r Langevin3, fig.height=4, fig.width=12}
##############################
# Testing Langevin for Volcano

# Tuning parameters
sigma = 0.25
trace1 = mcmc(x0, n, Lproposal, Lacceptance, myVolcano )
MCMCplots(trace1)

sigma = 0.75
trace2 = mcmc(x0, n, Lproposal, Lacceptance, myVolcano )
MCMCplots(trace2)

sigma = 1.5
trace3 = mcmc(x0, n, Lproposal, Lacceptance, myVolcano )
MCMCplots(trace3)

sigma = 2.5
trace4 = mcmc(x0, n, Lproposal, Lacceptance, myVolcano )
MCMCplots(trace4)

```
For resonably big choices of $\sigma=1.5$ a chain can be generated which does explore the entire ring with small autocorrelation and an acceptance rate close to the asymptotic optimum. For too small and too big choices the same as for the Random Walk holds: Only a fraction of the ring is explored for too small $\sigma=0.25$, then the chains starts to walk along the circle for $\sigma=0.5$. For too big proposal spread with $\sigma=2.5$ too many large steps will be rejected. 


### Conclusion

We saw that the MALA algorithm is very sensitive to small changes in tuning parameter $\sigma$. For too small choices it tends to get stuck in local maxima, what is a known issue and we realized that behaviour here as well. 

For the Gaussian distribution, the Langevin MCMC version works nicely, but the Random Walk does not perform bad here either. In the Multimodal case, the Random walk with suitable proposal spread does not get stuck in a maximum as the Langevin did. The circular density without clear mode is difficult for both MCMC versions, but the Langevin can be parametrised to handle is better. However, in general it depends a lot on the proper tuning for both.



## 1.4 Hamiltonian MH

```{r Hamiltonian}
K <- function(p){
    return(sum(t(p) %*% p) / 2)
}

HMC_acceptance <- function(x_prop, x_old, dens){
    U_old <- -log(dens(x_old))
    U_prop <- -log(dens(x_prop))
    alpha = min(1, exp(U_old - U_prop + K_old - K_prop))
    return(alpha)
}

HMC_proposal <- function(x_old, dens){
    x <- x_old
    p <- rnorm(length(x), 0, 1)
    p_old <- p
    res_leapfrog = leapfrog(p, x, delta, L, dU)
    p = - res_leapfrog[, 1]
    x = res_leapfrog[, 2]
    K_old = K(p_old)
    K_prop = K(p)
    print(K_old)
    print(K_prop)
    return(x)
}

leapfrog <- function(p, x, delta, L, dU){
    p <- p - delta * dU(x) / 2
    for (i in 1:L){
        x <- x + delta * p
        if (i != L) p <- p - delta * dU(x)
    }
    p <- p - delta * dU(x) / 2
    return(cbind(p, x))
}

U <- function(x){
    return(-log(dens(x)))
}

dU <- function(x_old){
    du = grad(U, x_old)
    if (any(is.nan(du))){
        du = c(0, 0)
    }
    return(du)
}

```







## Conclusion

- Burn-in
- All require tuning
- Compare performance



# 2 RStan

We consider the example of George et al. For a set of $N=10$ pumps we observe the values
- $y_i$: number of times that pump $i$ failed
- $t_i$: operation time of pump $i$
whose data is used as input in the next chunk.

```{r data}
# Define list with data input
data = list(
    N=10, 
    y=c(5,1,5,14,3,19,1,1,4,22), 
    t=c(94.3,15.7,62.9,126.0,5.24,31.4,1.05,1.05,2.1,10.5)
    )
```

The numbers of failures per pump are modelled by a $Poisson(\lambda_i t_i)$ likelihood together with a $Gamma(\alpha,\beta)$ distributed prior for the $\lambda_i$ with always $i=1,\dots,N$. Additionally,  the hyper-prior for $\alpha$ is $Exp(1.0)$ and for $\beta$ it is $Gamma(0.1,1.0)$ respectively. Therewith, we define a `stan` model. 

NB! We do not use a separate `.stan`-file but generate the model named "pump" directly in the environment. The syntax for the model generation is exactly the same as in files, but later the function call for the model fit deviates! The `pump.stan` file is only included for the seek of accordance with the exercise sheet.

```{stan output.var="pump"}
// generates a stan model named pump in the current environment

data{
  int<lower=0> N;       // number of pumps
  int<lower=0> y[N];    // number of failures
  real<lower=0> t[N];   // operation times of pumps
}

parameters{
  real<lower=0> lambda[N];
  real<lower=0> alpha;
  real<lower=0> beta;
}

transformed parameters{
  real<lower=0> eta[N];
  for (i in 1:N)
    eta[i] = lambda[i] * t[i];
}

model{
  target += exponential_lpdf( alpha | 1.0 );      // hyper-prior log-density
  target += gamma_lpdf( beta | 0.1, 1.0 );        // hyper-prior log-density
  target += gamma_lpdf( lambda | alpha, beta );   // prior log-density
  target += poisson_lpmf( y | eta );              // likelihood log-density
}

```

We sample from the posterior distribution using `stan`.

NB! The function call is different since we have already a stan model called "pump" in the cache and can sample from that directly.

```{r fitting}
# Load the RStan package
library(rstan)

# Fit the model using stan
fit <- sampling(
  pump,
  data = data,
  chains = 4,
  warmup = 1000,
  iter = 2000,
  seed = 1,
  control = list(adapt_delta = 0.9)
)
```

Finally, we investigate the results of the stan fit. 

```{r checking}
# Printing results of fit
print(fit)

# Plotting results of fit
# (Due to different scales we separate the plots)
plot(fit, pars="lambda")
plot(fit, pars=c("alpha", "beta"))
plot(fit, pars="eta")

traceplot(fit, pars="lambda", inc_warmup=TRUE, nrow=2)
traceplot(fit, pars=c("alpha","beta"), inc_warmup=TRUE, nrow=2)
traceplot(fit, pars="eta", inc_warmup=TRUE, nrow=2)
```

From the print output of the fitted model, we analyse the effective sample size `n_eff`. With only 4.000 post-warmup draws in total, for the $\lambda$ (and $\eta$) the effective samples size is around 5.000 and thereby surprisingly higher as the actual number of samples! In contrast for $\alpha$ and $\beta$, the effective sample size is only around 2500 and 3000, respectively, such that the quality of those samples is not as good as for the oother quantities. 

In the trace plots for $\lambda$ (and $eta$) we see that the state space is explored very effectively throughout the entire chains. Maybe for $i\in\{5,7,9\}$ the range of lower probability is not perfectly covered, but that is hard/impossible to say without further histograms. In general, we can assume that the chains reached a behaviour of the limit distribution already after a few dozen steps.
For $\alpha$ and $\beta$ we observe some higher autocorrelation and a less effective exploration of the target space. For a few dozen steps the sampels appear to remain close before the next jump comes, what is an undesired behavior.


