---
title: "Project1"
subtitle: "Markov Chain Monte Carlo techniques"
author: "Florian Beiser, Yaolin Ge"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Make Rmd stan compatible
# library(cmdstanr)
cmdstanr::register_knitr_engine()
```


# 1 Metropolis-Hastings for Bivariate Densities

We consider three different bivariate densities.
1. Standard Gaussian with correlation
2. Multimodal as muixture of three Gaussians
3. volcano (unnormalized)

## 1.1 Plotting

We start the project with the visulaisation of the respective densities in $[-5,5]\times[-5,5]$. (Whyever R likes to put more white space left and right of the plots, but the middle areas are still representative.)

```{r plotting}
# Auxilliary variables for visualisation
x=seq(-5,5,by=0.1)
y=seq(-5,5,by=0.1)

z1 = matrix(0, ncol=101, nrow=101)
z2 = matrix(0, ncol=101, nrow=101)
z3 = matrix(0, ncol=101, nrow=101)


##############################################
# 1. Standard Gaussian with Correlation

# Parameters
Sigma = matrix(c(1,0.9,0.9,1), nrow=2)
SigmaInv = solve(Sigma)
SigmaSqrt = sqrt(det(Sigma))

# Density
myGaussian <- function(x){
  f = 1/(2*pi*SigmaSqrt)*exp(-0.5*t(x)%*%SigmaInv%*%x)
  return(f)
}

# Fill z 
for (i in 1:101){
  for (j in 1:101){
    z1[i,j] = myGaussian(c(x[i],y[j]))
  }
}

# Visualise
image( x=x,
       y=y,
       z=z1,
       asp=1,
       main="Density of Standard Gaussian with Correlation",
       xlab="x", ylab="y")


##############################################
# 2. Multimodal

# Parameters
mu1 = c(-1.5,-1.5)
Sigma1 = matrix(c(1,0,0,1), ncol=2)
Sigma1Inv = solve(Sigma1)
Sigma1Sqrt = sqrt(det(Sigma1))

mu2 = c(1.5,1.5)
Sigma2 = matrix(c(1,0,0,1), ncol=2)
Sigma2Inv = solve(Sigma2)
Sigma2Sqrt = sqrt(det(Sigma2))

mu3 = c(-2,2)
Sigma3 = matrix(c(0.8,0,0,0.8), ncol=2)
Sigma3Inv = solve(Sigma3)
Sigma3Sqrt = sqrt(det(Sigma3))

# Density
myMultimodal <- function(x){
  f = 1/3 * 1/(2*pi*Sigma1Sqrt)*exp(-0.5*t(x-mu1)%*%Sigma1Inv%*%(x-mu1))
  f = f + 1/3 * 1/(2*pi*Sigma2Sqrt)*exp(-0.5*t(x-mu2)%*%Sigma2Inv%*%(x-mu2))
  f = f + 1/3 * 1/(2*pi*Sigma3Sqrt)*exp(-0.5*t(x-mu3)%*%Sigma3Inv%*%(x-mu3))
  return(f)
}

# Fill z
for (i in 1:101){
  for (j in 1:101){
    z2[i,j] = myMultimodal(c(x[i],y[j]))
  }
}

# Visualise
image( x=x,
       y=y,
       z=z2,
       asp=1,
       main="Density of Multimodal",
       xlab="x", ylab="y")


##############################################
# 3. Volcano

# Density
myVolcano <- function(x){
  f = 1/(2*pi)*exp(-0.5*t(x)%*%x)*(t(x)%*%x+0.25)
  return(f)
}

# Fill z 
for (i in 1:101){
  for (j in 1:101){
    z3[i,j] = myVolcano(c(x[i],y[j]))
  }
}

# Visualise
image( x=x,
       y=y,
       z=z3,
       asp=1,
       main="Density of Volcano",
       xlab="x", ylab="y")

```

For the subsequent part, where different Metropolis Hastings MCMC algorithms will be implemented, we prepare with the skeleton that was discussed in the exercise class and uploaded. This defines the frame for all MCMC implementations under consideration which differ in the `proposal_func` and `acceptance_func`.  

```{r MCMC}
# Arguments:
# x0 = vector of length 2 with initial values for x
# n = number of MCMC iterations to run
# proposal_func = function returning a vector of length 2 which is 
#                 the new proposal
# accept_func = function computing the acceptance prob. alpha
# dens = function defining your target density
mcmc <- function(x0, n, proposal_func, acceptance_func, dens) {
  
  # object to save MCMC samples
  x <- matrix(0, nrow=n, ncol=2)
  alphas <- rep(0,n)
  # initialisation
  x_old <- x0
  # generate a vector of n uniform distributed random variables
  u <- runif(n)
  # go through n itearations
  for (i in 1:n) {
    # get a new proposal for x
    x_prop <- proposal_func(x_old)
    # compute the acceptance prob alpha
    alpha <- acceptance_func(x_prop, x_old, dens)
    alphas[i] = alpha
    # decide wether to accept or reject
    if (u[i] < alpha) {
      x_new <- x_prop
    } else {
      x_new <- x_old
    }
    # save the new sample
    x[i,] <- x_new
    x_old <- x_new
  }
  return(list("x"=x,"alphas"=alphas))
}
```

These implementation is not tweaked for optimal performance, e.g. log-scale could lead to better scaling or reduction of redundant calculations in the individual MCMC versions could lead to fast execution, but we want to the same skeleton for all algorithms to higlight similarities and differences. For the analysis of the outcomes, we use below's plotting functionality.

```{r MCMCplotting}
# Arguments:
# trace = trace of MCMC algo [x_1,x_2]^n

# Function:
# - Density plots
# - Trace plots
# - Autocorrelation
MCMCplots <- function(trace, surface=FALSE){
  # Generating empirical densitty
  kd = MASS::kde2d(x=trace$x[,1], y=trace$x[,2], n=101, lims=c(-5,5,-5,5))
  
  # Nice 3D empirical density plots 
  # WARNING: VERY SLOW
  if (surface==TRUE){
    library(plotly)
    plot_ly(x=kd$x, y=kd$y, z=kd$z, type="surface")
  }
  
  # 2D empirical density plots
  image( x=kd$x, y=kd$y, z=kd$z,
         asp=1, main="Density",
         xlab="x", ylab="y", xlim=c(-5,5), ylim=c(-5,5))
  
  # Trace plots
  plot(trace$x[,1], type="l", ylab="x1", 
     main=paste("Trace of x1 (tuning parameter",sigma,")"))
  plot(trace$x[,2], type="l", ylab="x1", 
     main=paste("Trace of x2 (tuning parameter",sigma,")"))

  # Autocorrelation
  acf(trace$x[,1], main="Trace plot for x1")
  acf(trace$x[,2], main="Trace plot for x2")
  
  # Mean acceptance rate
  print(paste("The mean acceptance rate is", sum(trace$alphas)/length(trace$alphas)))
}

```


## 1.2 Random walk MH 

The random walk MH uses a symmetric $\mathcal{N}(\texttt{x_old},\sigma^2)$ distribution to generate the new proposal. Moreover, the general MH acceptance probability simplifies.

```{r RandomWalk}
# Arguments: 
# x_old = old state [x_1_old, x_2_old]

# Function:
# Generating proposal following the Random walk MH

# Return:
# x_prop = proposed state [x_1_prop, x_2_prop]
RWproposal <- function(x_old){
  x_prop = c(rmvnorm(1, x_old, sigma**2 * diag(1,2,2)))
  return(x_prop)
}

# Arguments: 
# x_prop = proposed state [x_1_prop, x_2_prop]
# x_old = old state [x_1_old, x_2_old]
# dens = density function of target dist

# Function:
# Calculating acceptance prob for random walk MH

# Return:
# alpha = acceptance probability (numeric)
RWacceptance <- function(x_prop, x_old, dens){
  alpha = min(1, dens(x_prop)/dens(x_old))
  return(alpha)
}
```

### Standard Gaussian

We first test the Random Walk MH for the Gaussian with different tuning parameters $\sigma$ in the proposal distribution. 

```{r RandomWalk1}

##############################
# Testing Random Walk for Gaussian

# Input
x0 = c(0,0)
n = 2000

# Tuning parameters
sigma = 0.1
trace0 = mcmc(x0, n, RWproposal, RWacceptance, myGaussian )
MCMCplots(trace0)

sigma = 0.5 
trace1 = mcmc(x0, n, RWproposal, RWacceptance, myGaussian )
MCMCplots(trace1)

sigma = 1.5
trace2 = mcmc(x0, n, RWproposal, RWacceptance, myGaussian )
MCMCplots(trace2)

sigma = 5.0
trace3 = mcmc(x0, n, RWproposal, RWacceptance, myGaussian )
MCMCplots(trace3)

```

Depending on the choice of the tuning parameter the MCMC algorithms show different efficiency. Actually all tuning parameters explore the state-space rather sedately. However, for values $\sigma<0.5$ the mean acceptance rate gets bigger than recommended and the traces change in too little step, for values $\sigma>1.5$ the traces start to pause in a level due to a too small acceptance rate. We recommend values $\sigma\in(0.5,1.5)$ for the Gaussian, since for those values the autocorrelation has t he smallest lag and shrinks at least after 20 steps (what is still much).


### Multimodal

We continue with application of the Random Walk MH to the Multimodal with different tuning parameters $\sigma$ in the proposal distribution. 

```{r RandomWalk2}

##############################
# Testing Random Walk for Multimodal

# Input
x0 = c(0,0)
n = 2000

# Tuning parameters
sigma = 0.1
trace1 = mcmc(x0, n, RWproposal, RWacceptance, myMultimodal )
MCMCplots(trace1)

sigma = 1.0
trace2 = mcmc(x0, n, RWproposal, RWacceptance, myMultimodal )
MCMCplots(trace2)

sigma = 5.0
trace3 = mcmc(x0, n, RWproposal, RWacceptance, myMultimodal )
MCMCplots(trace3)

sigma = 10.0
trace3 = mcmc(x0, n, RWproposal, RWacceptance, myMultimodal )
MCMCplots(trace3)

```

For the smallest choice of $\sigma=0.1$ the chain does not really explore all modes and stays for quite long in one mode when it is there, therefore the autocorrelation is very high. When increasing up to $\sigma=5.0$ we improve the exploration of the different modes and reduce the autocorrelation significantly. Moreover, the mean acceptance rate reaches the recommended range. For very high tuning parameter choices $\sigma=10.0$, we start to wildly jump from one mode to the other, but the traces start to pause too long between the jumps. 


### Volcano

Finally, we try the random walk MH for the volcano shaped density.

```{r RandomWalk3}

##############################
# Testing Random Walk for Multimodal

# Input
x0 = c(0,0)
n = 2000

# Tuning parameters
sigma = 0.1
trace1 = mcmc(x0, n, RWproposal, RWacceptance, myVolcano )
MCMCplots(trace1)

sigma = 0.5
trace2 = mcmc(x0, n, RWproposal, RWacceptance, myVolcano )
MCMCplots(trace2)

sigma = 1.5
trace3 = mcmc(x0, n, RWproposal, RWacceptance, myVolcano )
MCMCplots(trace3)

sigma = 1.5
trace3 = mcmc(x0, n, RWproposal, RWacceptance, myVolcano )
MCMCplots(trace3)

```

If the tuning parameter is chosen too small $\sigma=0.1$ the chain does not explore the entire ring, but get stuck. When increasing $\sigma$ the chain starts to walk along the circle, but for higher parameters like $\sigma=1.5$ the chain explores the ring with a short autocorrelation (figuratively speaking, it can also jump from one side toe the other and does not need to walk along the circle). 


### Conclusion

For all examples, there is a range of runing parameters which explore the state-space, but the autocorrelation is still pretty high.



## 1.3 Langevin MH

Based on the Langevin dynamics and its Euler-Maruyama discretisation, the MALA algorithm uses gradient information to define the proposal density. 

```{r Langevin}
# Arguments:
# x_old = old state [x_1,x_2]
# dens = target density (from local variables)

# Function:
# Using 2nd order central-FD 

# Return:
# Lgrad = grad (log (dens(x_old)))
Lgrad <- function(x_old){
  Lgrad = rep(0,length(x_old))
  h = 1e-3
  e1 = c(1,0)
  e2 = c(0,1)
  Lgrad[1] = (-log(dens(x_old-h*e1)) + log(dens(x_old+h*e1)))/(2*h)
  Lgrad[2] = (-log(dens(x_old-h*e2)) + log(dens(x_old+h*e2)))/(2*h)
  return(Lgrad)
}

# Arguments: 
# x_old = old state [x_1_old, x_2_old]

# Function:
# Generating proposal following the Lnagevin MH

# Return:
# x_prop = proposed state [x_1_prop, x_2_prop]
Lproposal <- function(x_old){
  x_prop =c(rmvnorm(1, x_old + sigma**2/2 * Lgrad(x_old), sigma**2 *diag(1,2,2))) 
  return(x_prop)
}

# Arguments: 
# x = state [x_1, x_2]
# y = condition [y_1,y_2]

# Function:
# Calculating conditional proposal density value 
# for the Langevin proposal Q

# Return:
# f = Q(x|y)
dLproposal <- function(x,y){
  f = dmvnorm(x, y + sigma**2/2 * Lgrad(y), sigma**2 *diag(1,2,2))
  return(f)
}


# Arguments: 
# x_prop = proposed state [x_1_prop, x_2_prop]
# x_old = old state [x_1_old, x_2_old]
# dens = density function of target dist

# Function:
# Calculating acceptance prob for Langevin MH

# Return:
# alpha = acceptance probability (numeric)
Lacceptance <- function(x_prop, x_old, dens){
  alpha = min(1, dLproposal(x_old, x_prop)/dLproposal(x_prop,x_old) * dens(x_prop)/dens(x_old))
  return(alpha)
}
```

### Standard Gaussian

We first test the Langevin MH for the Gaussian with different tuning parameters $\sigma$ in the proposal distribution. 

```{r Langevin1}

##############################
# Testing Langevin for Gaussian

# Input
x0 = c(0,0)
n = 2000

# Tuning parameters
sigma = 0.1
trace0 = mcmc(x0, n, Lproposal, Lacceptance, myGaussian )
MCMCplots(trace0)

sigma = 0.5 
trace1 = mcmc(x0, n, Lproposal, Lacceptance, myGaussian )
MCMCplots(trace1)

sigma = 1.0
trace2 = mcmc(x0, n, Lproposal, Lacceptance, myGaussian )
MCMCplots(trace2)

```

### Multimodal

We continue with application of the Langevin MH to the Multimodal with different tuning parameters $\sigma$ in the proposal distribution. 

```{r Langevin2}

##############################
# Testing Langevin for Multimodal

# Input
x0 = c(0,0)
n = 2000

# Tuning parameters
sigma = 0.25
trace1 = mcmc(x0, n, Lproposal, Lacceptance, myMultimodal )
MCMCplots(trace1)

sigma = 0.5
trace2 = mcmc(x0, n, Lproposal, Lacceptance, myMultimodal )
MCMCplots(trace2)

sigma = 0.75
trace3 = mcmc(x0, n, Lproposal, Lacceptance, myMultimodal )
MCMCplots(trace3)

```

### Volcano

Finally, we try the Langevin MH for the volcano shaped density.

```{r Langevin3}

##############################
# Testing Langevin for Multimodal

# Input
x0 = c(0,0)
n = 2000

# Tuning parameters
sigma = 0.1
trace1 = mcmc(x0, n, Lproposal, Lacceptance, myVolcano )
MCMCplots(trace1)

sigma = 0.25
trace2 = mcmc(x0, n, Lproposal, Lacceptance, myVolcano )
MCMCplots(trace2)

sigma = 0.5
trace3 = mcmc(x0, n, Lproposal, Lacceptance, myVolcano )
MCMCplots(trace3)

sigma = 0.75
trace3 = mcmc(x0, n, Lproposal, Lacceptance, myVolcano )
MCMCplots(trace3)

```


### Conclusion

Damn it is sensitive to the parameters! :-P



## 1.4 Hamiltonian MH

```{r Hamiltonian}
K <- function(p){
    return(sum(t(p) %*% p) / 2)
}

# Gaussian
U1 <- function(x){
    U = t(x) %*% solve(matrix(c(1,0.9,0.9,1), nrow=2)) %*% x
    return(U)
}

dU1 <- function(x){
    dU = t(x) %*% solve(matrix(c(1,0.9,0.9,1), nrow=2))
    return(dU)
}

# Multimodal
U2 <- function(x){
    mu1 = c(-1.5,-1.5)
    mu2 = c(1.5,1.5)
    mu3 = c(-2,2)
    U = t(x - mu1) %*% solve(matrix(c(1,0,0,1), ncol=2)) %*% (x - mu1) + 
        t(x - mu2) %*% solve(matrix(c(1,0,0,1), ncol=2)) %*% (x - mu2) + 
        t(x - mu3) %*% solve(matrix(c(0.8,0,0,0.8), ncol=2)) %*% (x - mu3)
    return(U)
}

dU2 <- function(x){
    mu1 = c(-1.5,-1.5)
    mu2 = c(1.5,1.5)
    mu3 = c(-2,2)
    dU = t(x - mu1) %*% solve(matrix(c(1,0,0,1), nrow=2)) + 
         t(x - mu2) %*% solve(matrix(c(1,0,0,1), nrow=2)) + 
         t(x - mu3) %*% solve(matrix(c(0.8,0,0,0.8), nrow=2)) 
    return(dU)
}

# Volcano
U3 <- function(x){
    U = -1/2 * t(x) %*% x * log(t(x) %*% x + 0.25)
    return(U)
}
dU3 <- function(x){
    dU = -t(x) %*% log(x %*% t(x) + 0.25) - 1/2 * t(x) %*% x %*% t(x) %*% solve(x %*% t(x) + 0.25)
    return(dU)
}

# U is a function that returns the potential energy given q
# grad_U returns the respective partial derivatives
# epsilon stepsize
# L number of leapfrog steps
# current_q current position

# kinetic energy is assumed to be sum(p^2/2) (mass == 1)
HMC <- function (U, dU, delta, L, current_q) {
    q <- current_q
    # independent standard normal variates
    p <- t(rnorm(length(q), 0, 1))
    # Make a half step for momentum at the beginning
    current_p <- p 
    # Alternate full steps for position and momentum
    p <- p - delta * dU(q) / 2 
    for (i in 1:L) {
        # Make a full step for the position
        q <- q + delta * t(p)
        # Make a full step for the momentum, except at end of trajectory
        if (i != L) p <- p - delta * dU(q)
    }
    # Make a half step for momentum at the end
    p <- p - delta * dU(q) / 2
    # Negate momentum at end of trajectory to make the proposal symmetric
    p <- -p
    # Evaluate potential and kinetic energies at start and end of trajectory 
    current_U <- U(current_q)
    current_K <- K(current_p)  #sum(current_p^2) / 2
    proposed_U <- U(q)
    proposed_K <- K(p) #sum(p^2) / 2
    # Accept or reject the state at end of trajectory, returning either
    # the position at the end of the trajectory or the initial position
    if (runif(1) < exp(current_U-proposed_U+current_K-proposed_K)) {
        return (q)  # accept
    } else {
        return (current_q)  # reject
    }
}

# setup
delta = 0.1
nSamples = 1000
L = 10

# initialisation
x = matrix(0, 2, nSamples)
x[, 1] = c(0, 6)
t = 1
while(t < nSamples){
    t = t + 1
    # x[, t] = HMC(U = U1, dU = dU1, delta = delta, L = L, current_q = t(t(x[, t-1])))
    # x[, t] = HMC(U = U2, dU = dU2, delta = delta, L = L, current_q = t(t(x[, t-1])))
    x[, t] = HMC(U = U2, dU = dU2, delta = delta, L = L, current_q = t(t(x[, t-1])))
}

plot(x[1, ])


```








# 2 RStan

We consider the example of George et al. For a set of $N=10$ pumps we observe the values
- $y_i$: number of times that pump $i$ failed
- $t_i$: operation time of pump $i$
whose data is used as input in the next chunk.

```{r data}
# Define list with data input
data = list(
    N=10, 
    y=c(5,1,5,14,3,19,1,1,4,22), 
    t=c(94.3,15.7,62.9,126.0,5.24,31.4,1.05,1.05,2.1,10.5)
    )
```

The numbers of failures per pump are modelled by a $Poisson(\lambda_i t_i)$ likelihood together with a $Gamma(\alpha,\beta)$ distributed prior for the $\lambda_i$ with always $i=1,\dots,N$. Additionally,  the hyper-prior for $\alpha$ is $Exp(1.0)$ and for $\beta$ it is $Gamma(0.1,1.0)$ respectively. Therewith, we define a `stan` model. 

NB! We do not use a separate `.stan`-file but generate the model named "pump" directly in the environment. The syntax for the model generation is exactly the same as in files, but later the function call for the model fit deviates! The `pump.stan` file is only included for the seek of accordance with the exercise sheet.

```{stan output.var="pump"}
// generates a stan model named pump in the current environment

data{
  int<lower=0> N;       // number of pumps
  int<lower=0> y[N];    // number of failures
  real<lower=0> t[N];   // operation times of pumps
}

parameters{
  real<lower=0> lambda[N];
  real<lower=0> alpha;
  real<lower=0> beta;
}

transformed parameters{
  real<lower=0> eta[N];
  for (i in 1:N)
    eta[i] = lambda[i] * t[i];
}

model{
  target += exponential_lpdf( alpha | 1.0 );      // hyper-prior log-density
  target += gamma_lpdf( beta | 0.1, 1.0 );        // hyper-prior log-density
  target += gamma_lpdf( lambda | alpha, beta );   // prior log-density
  target += poisson_lpmf( y | eta );              // likelihood log-density
}

```

We sample from the posterior distribution using `stan`.

NB! The function call is different since we have already a stan model called "pump" in the cache and can sample from that directly.

```{r fitting}
# Load the RStan package
library(rstan)

# Fit the model using stan
fit <- sampling(
  pump,
  data = data,
  chains = 4,
  warmup = 1000,
  iter = 2000,
  seed = 1,
  control = list(adapt_delta = 0.9)
)
```

Finally, we investigate the results of the stan fit. 

```{r checking}
# Printing results of fit
print(fit)

# Plotting results of fit
# (Due to different scales we separate the plots)
plot(fit, pars="lambda")
plot(fit, pars=c("alpha", "beta"))
plot(fit, pars="eta")

traceplot(fit, pars="lambda", inc_warmup=TRUE, nrow=2)
traceplot(fit, pars=c("alpha","beta"), inc_warmup=TRUE, nrow=2)
traceplot(fit, pars="eta", inc_warmup=TRUE, nrow=2)
```

From the print output of the fitted model, we analyse the effective sample size `n_eff`. With only 4.000 post-warmup draws in total, for the $\lambda$ (and $\eta$) the effective samples size is around 5.000 and thereby surprisingly higher as the actual number of samples! In contrast for $\alpha$ and $\beta$, the effective sample size is only around 2500 and 3000, respectively, such that the quality of those samples is not as good as for the oother quantities. 

In the trace plots for $\lambda$ (and $eta$) we see that the state space is explored very effectively throughout the entire chains. Maybe for $i\in\{5,7,9\}$ the range of lower probability is not perfectly covered, but that is hard/impossible to say without further histograms. In general, we can assume that the chains reached a behaviour of the limit distribution already after a few dozen steps.
For $\alpha$ and $\beta$ we observe some higher autocorrelation and a less effective exploration of the target space. For a few dozen steps the sampels appear to remain close before the next jump comes, what is an undesired behavior.


